{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL1bYBTHdyJm"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Connection"
      ],
      "metadata": {
        "id": "bf5O4rMPAqmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BvYQflVdxnH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WduEBxf04_yl"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/QA-CHATBOT-LLM/RAG\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9EHzNtmb0-t"
      },
      "source": [
        "# Libraries installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRCPGPiQbwxG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install sentence_transformers==2.2.2\n",
        "!pip install huggingface-hub==0.25.0\n",
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "! pip install -qq -U tiktoken\n",
        "! pip install -qq -U pypdf\n",
        "! pip install -qq -U faiss-gpu\n",
        "! pip install -qq -U InstructorEmbedding\n",
        "! pip install -qq -U accelerate\n",
        "! pip install -qq -U bitsandbytes\n",
        "! pip install -qq -U langchain_community\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7XALphQb4lU"
      },
      "source": [
        "# Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i4-oGKZb5CV"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import textwrap\n",
        "import time\n",
        "\n",
        "import langchain\n",
        "\n",
        "### loaders\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "\n",
        "### splits\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "### prompts\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "### vector stores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "### models\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "### retrievers\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P42snW-Rb8PF"
      },
      "outputs": [],
      "source": [
        "print('langchain:', langchain.__version__)\n",
        "print('torch:', torch.__version__)\n",
        "print('transformers:', transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iBdLSFjcADV"
      },
      "source": [
        "# Environment configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M95QMwQ5cDeL"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    # LLMs\n",
        "    model_name = 'Llama' #'Mistral'\n",
        "    temperature = 0\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.15\n",
        "\n",
        "    # splitting\n",
        "    split_chunk_size = 800\n",
        "    split_overlap = 0\n",
        "\n",
        "    # embeddings\n",
        "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "    # similar passages\n",
        "    k = 6\n",
        "\n",
        "    # paths\n",
        "    input_path = '/content/gdrive/MyDrive/QA-CHATBOT-LLM/RAG/Dataset/InputDataFormatted/'\n",
        "    Embeddings_path = 'qa-chatbot-llm-vectordb/faiss_index_hp'\n",
        "    output_path = './qa-chatbot-llm-vectordb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqDUeI-XcIDN"
      },
      "source": [
        "#Model definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkwTTqpBcLEb"
      },
      "outputs": [],
      "source": [
        "def get_model(model = CFG.model_name):\n",
        "\n",
        "    print('\\nDownloading model: ', model, '\\n\\n')\n",
        "\n",
        "    if model == 'Mistral':\n",
        "        model_repo = 'Moxoff/Mistral-Ita' #'Moxoff/Mistral-Ita' #'mistral-7B' #'Mistral-Ita-7b'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    elif model == 'Llama':\n",
        "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048 # 8192\n",
        "\n",
        "    else:\n",
        "        print(\"Not implemented model (tokenizer and backbone)\")\n",
        "\n",
        "    return tokenizer, model, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AnqFzFncNOD"
      },
      "source": [
        "# HuggingFace access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjmQmrgjcOez"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "secret_hf = userdata.get('HUGGINGFACE_TOKEN')\n",
        "!huggingface-cli login --token $secret_hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBfGEgAxcVN_"
      },
      "source": [
        "# Model access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z275eLi3cYAd"
      },
      "outputs": [],
      "source": [
        "tokenizer, model, max_len = get_model(model = CFG.model_name)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2W0ETSecZFN"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4NetSiScbIH"
      },
      "outputs": [],
      "source": [
        "### check how Accelerate split the model across the available devices (GPUs)\n",
        "model.hf_device_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyP4aTFMcecF"
      },
      "outputs": [],
      "source": [
        "### hugging face pipeline\n",
        "pipe = pipeline(\n",
        "    task = \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "#   do_sample = True,\n",
        "    max_length = max_len,\n",
        "    temperature = CFG.temperature,\n",
        "    top_p = CFG.top_p,\n",
        "    repetition_penalty = CFG.repetition_penalty\n",
        ")\n",
        "\n",
        "### langchain pipeline\n",
        "llm = HuggingFacePipeline(pipeline = pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPcjqcyncgXd"
      },
      "outputs": [],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU7VUUvvdGD0"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdCqtbVVx9hF"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "def get_loader(source):\n",
        "\n",
        "    loader = DirectoryLoader(\n",
        "        CFG.input_path,\n",
        "        glob=\"./\" + source + \".txt\",\n",
        "        loader_cls=TextLoader,\n",
        "        show_progress=True,\n",
        "        use_multithreading=True\n",
        "    )\n",
        "\n",
        "    documents = loader.load()\n",
        "    print(f'Loading sample dataset\\n')\n",
        "    print(f'We have {len(documents)} total page')\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Split in chunk\n",
        "def chunk_split(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = CFG.split_chunk_size,\n",
        "        chunk_overlap = CFG.split_overlap\n",
        "    )\n",
        "\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f'{len(texts)} chunk of {len(documents)} pages created')\n",
        "    return texts\n",
        "\n",
        "\n",
        "# Delete a folder\n",
        "def clear_folder(path):\n",
        "    if os.path.exists(CFG.Embeddings_path + '/index.faiss'):\n",
        "        for item in os.listdir(path):\n",
        "            item_path = os.path.join(path, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                pass\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "                print(\"Deleted file: \" + item_path)\n",
        "\n",
        "\n",
        "# Create embeddings\n",
        "def create_embeddings(path, texts):\n",
        "\n",
        "    clear_folder(path)\n",
        "\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name = CFG.embeddings_model_repo,\n",
        "        model_kwargs = {\"device\": \"cuda\"}\n",
        "    )\n",
        "\n",
        "    vectordb = FAISS.from_documents(\n",
        "        documents = texts,\n",
        "        embedding = embeddings\n",
        "    )\n",
        "    print(\"Embeddings created at: \" + path)\n",
        "    return vectordb.save_local(path)\n",
        "\n",
        "\n",
        "# Load vectordb\n",
        "def load_vectordb(vectordb):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name = CFG.embeddings_model_repo,\n",
        "        model_kwargs = {\"device\": \"cuda\"}\n",
        "    )\n",
        "\n",
        "    ### load vector DB embeddings\n",
        "    vectordb = FAISS.load_local(\n",
        "        CFG.Embeddings_path,\n",
        "        embeddings,\n",
        "        allow_dangerous_deserialization=True #Attenzione!\n",
        "    )\n",
        "    print(\"vectordb loaded\")\n",
        "    clear_output()\n",
        "    return vectordb\n",
        "\n",
        "# Create retriever\n",
        "def create_retriever(vectordb, k, search_type):\n",
        "    return vectordb.as_retriever(search_kwargs = {\"k\": k, \"search_type\" : search_type})\n",
        "\n",
        "\n",
        "# Prompt definition\n",
        "prompt_template = \"\"\"\n",
        "Answer specifically only the input question and do not answer the questions present in the context.\n",
        "If the answer is not provided in the context, simply write \"Answer not available in the context\", and do not provide the wrong answer.\n",
        "Use only the following context to answer the final question.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template = prompt_template,\n",
        "    input_variables = [\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create qa_chain\n",
        "def create_qa_chain(llm, retriever, PROMPT):\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm = llm,\n",
        "        chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
        "        retriever = retriever,\n",
        "        chain_type_kwargs = {\"prompt\": PROMPT},\n",
        "        return_source_documents = True,\n",
        "        verbose = False\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "# Output post-processing\n",
        "def wrap_text_preserve_newlines(text, width=700):\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "    return wrapped_text\n",
        "\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
        "    sources_used = ' \\n'.join(\n",
        "        [\n",
        "            source.metadata['source'].split('/')[-1][:-4]\n",
        "            for source in llm_response['source_documents']\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return ans\n",
        "\n",
        "\n",
        "def extract_text_after_substring(s, substring1):\n",
        "    index = s.find(substring1)\n",
        "    if index != -1:\n",
        "        return s[index + len(substring1):]\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# LLM response\n",
        "def llm_ans(qa_chain, query):\n",
        "    start = time.time()\n",
        "    llm_response = qa_chain.invoke(query)\n",
        "    ans = process_llm_response(llm_response)\n",
        "    end = time.time()\n",
        "    time_elapsed = int(round(end - start, 0))\n",
        "    return ans\n",
        "\n",
        "# Output\n",
        "def get_result(source, query):\n",
        "    documents = get_loader(source)\n",
        "    texts = chunk_split(documents)\n",
        "    output_path =  f\"{CFG.output_path}/faiss_index_hp\" #<path_destinazione_embeddings>\n",
        "    vectordb = create_embeddings(output_path, texts)\n",
        "    #da testare\n",
        "    vectordb = load_vectordb(vectordb)\n",
        "    retriever = create_retriever(vectordb, CFG.k, \"similarity\")\n",
        "    qa_chain = create_qa_chain(llm, retriever, PROMPT)\n",
        "    ans = llm_ans(qa_chain, query)\n",
        "    #print(ans)\n",
        "    return extract_text_after_substring(ans, \"Answer:\")\n",
        "\n",
        "print(get_result(\"Machine-Learning\", \"How many type of Machine Learning exist?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaMnzd09I_H1"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "! pip install --upgrade gradio -qq\n",
        "\n",
        "import gradio as gr\n",
        "print(gr.__version__)\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def respond_to_message(state, item, message):\n",
        "    # Generate a response based on the selected item and the message\n",
        "    response = get_result(item, message)\n",
        "\n",
        "    state.append((message, response))  # Append the user message and the response to the chat state\n",
        "    return state, gr.update(visible=False), \"\"  # Return the updated state for the chat and the new state\n",
        "\n",
        "def reset_chat(state):\n",
        "    return [], [], \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "\n",
        "    state = gr.State([])  # Initialize the chat state as an empty list\n",
        "\n",
        "    with gr.Tab(\"💬 QA Chatbot\"):\n",
        "        gr.Markdown(\"## 🗣️ Hi, how can I help you?\")\n",
        "        with gr.Row():\n",
        "            item_dropdown = gr.Dropdown([\"World-History\", \"Machine-Learning\", \"Health-and-Wellness\"], label=\"Select your subject\")\n",
        "        with gr.Row():\n",
        "            direct_chatbot = gr.Chatbot(label=\"💬 Chat\")\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=5):\n",
        "                direct_textbox = gr.Textbox(placeholder=\"💭 Insert your message here...\")\n",
        "            direct_submit_btn = gr.Button(value=\"Send\")\n",
        "        with gr.Row():\n",
        "            #direct_regenerate_btn = gr.Button(value=\"🔄 Regenerate\")\n",
        "            direct_reset_btn = gr.Button(value=\"🗑️ Reset Chat\")\n",
        "\n",
        "\n",
        "    # Define the action for the submit button (pressed button)\n",
        "    direct_submit_btn.click(\n",
        "        respond_to_message,\n",
        "        inputs=[state, item_dropdown, direct_textbox],  # Inputs to the function: the current state and the message from the textbox\n",
        "        outputs=[direct_chatbot, state, direct_textbox]  # Outputs: updated chatbot, warning message, and state\n",
        "    )\n",
        "\n",
        "    # Define the action for the submit button (enter from keyboard)\n",
        "    direct_textbox.submit(\n",
        "        respond_to_message,\n",
        "        inputs=[state, item_dropdown, direct_textbox],  # Inputs: state, selected model, and message\n",
        "        outputs=[direct_chatbot, state, direct_textbox]  # Outputs: updated chatbot, warning message, and state\n",
        "    )\n",
        "\n",
        "    direct_reset_btn.click(\n",
        "        reset_chat,\n",
        "        inputs=[],\n",
        "        outputs=[direct_chatbot, state, direct_textbox]  # Outputs: updated chatbot, warning message, state, and textbox\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}